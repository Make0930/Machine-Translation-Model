{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T19:36:46.995731Z",
     "start_time": "2020-11-16T19:36:46.982775Z"
    }
   },
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T19:38:06.072293Z",
     "start_time": "2020-11-16T19:38:06.065790Z"
    }
   },
   "source": [
    "This model is based on encoder-decoder structure. And is uesd for French-English translation tasks.\n",
    "\n",
    "The model added the attention mechanism, and used two generation methods: greedy and beam search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T19:38:45.995544Z",
     "start_time": "2020-11-16T19:38:45.992694Z"
    }
   },
   "source": [
    "# 2.Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T19:38:54.930315Z",
     "start_time": "2020-11-16T19:38:54.927323Z"
    }
   },
   "source": [
    "## 2.1 Import related packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T21:16:04.097255Z",
     "start_time": "2020-11-16T21:16:04.093152Z"
    }
   },
   "outputs": [],
   "source": [
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "#Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as DataSet\n",
    "\n",
    "# draw\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# GPU\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T19:39:15.971695Z",
     "start_time": "2020-11-16T19:39:15.968690Z"
    }
   },
   "source": [
    "## 2.2Prepare parallel corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T21:16:04.995380Z",
     "start_time": "2020-11-16T21:16:04.926928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135842\n",
      "135842\n"
     ]
    }
   ],
   "source": [
    "# Define two special symbols, corresponding to the beginning and end of the sentence\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "lines = open('fra.txt', encoding = 'utf-8')\n",
    "french = lines.read().strip().split('\\n')\n",
    "lines = open('eng.txt', encoding = 'utf-8')\n",
    "english = lines.read().strip().split('\\n')\n",
    "print(len(french))\n",
    "print(len(english))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T21:16:05.765646Z",
     "start_time": "2020-11-16T21:16:05.760137Z"
    }
   },
   "outputs": [],
   "source": [
    "# Two dictionaries：word2index，index2word\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        # Add a new sentence to the language, a sentence is a set of words separated by spaces\n",
    "        # Segment the words and process them separately\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        # Insert a word, if the word is already in the dictionary, update the frequency of the corresponding word in the dictionary\n",
    "        # Create a reverse index at the same time, you can find the word from the word number\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T21:16:06.501730Z",
     "start_time": "2020-11-16T21:16:06.489601Z"
    }
   },
   "outputs": [],
   "source": [
    "# Turn a Unicode string to plain ASCII\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Remove special symbols and convert English strings to lowercase\n",
    "def normalizeEngString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s\n",
    "\n",
    "# Filter the input word pairs to ensure that the number of words in each sentence cannot exceed MAX_LENGTH\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "# Input a sentence, output a code sequence corresponding to a word\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "\n",
    "# Similar to the function above, the difference is that the output sequence is the same length = MAX_LENGTH\n",
    "def indexFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    for i in range(MAX_LENGTH - len(indexes)):\n",
    "        indexes.append(EOS_token)\n",
    "    return(indexes)\n",
    "\n",
    "# From a word pair to a subscript\n",
    "def indexFromPair(pair):\n",
    "    input_variable = indexFromSentence(input_lang, pair[0])\n",
    "    target_variable = indexFromSentence(output_lang, pair[1])\n",
    "    return (input_variable, target_variable)\n",
    "\n",
    "def SentenceFromList(lang, lst):\n",
    "    result = [lang.index2word[i] for i in lst if i != EOS_token]\n",
    "    if lang.name == 'French':\n",
    "        result = ' '.join(result)\n",
    "    else:\n",
    "        result = ' '.join(result)\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T21:16:07.230041Z",
     "start_time": "2020-11-16T21:16:07.226057Z"
    }
   },
   "outputs": [],
   "source": [
    "# Functions for calculating accuracy\n",
    "def rightness(predictions, labels):\n",
    "    # For the first dimension of the output value of any row (a sample), find the maximum and get the subscript of the largest element in each row\n",
    "    pred = torch.max(predictions.data, 1)[1] \n",
    "    #Compare the subscripts with the categories contained in the labels, and accumulate the correct number\n",
    "    rights = pred.eq(labels.data).sum() \n",
    "    #Return the correct number and how many elements have been compared this time\n",
    "    return rights, len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T19:42:03.420707Z",
     "start_time": "2020-11-16T19:42:03.417569Z"
    }
   },
   "source": [
    "## 2.3 Split Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T21:16:12.390325Z",
     "start_time": "2020-11-16T21:16:08.350444Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid sentence pairs： 5423\n",
      "Total words:\n",
      "French 2851\n",
      "English 1703\n",
      "Training   set samples： 4881\n",
      "Validation set samples： 271\n",
      "Test       set samples： 271\n"
     ]
    }
   ],
   "source": [
    "# Set the maximum length of sentences\n",
    "MAX_LENGTH = 5\n",
    "\n",
    "pairs = [[normalizeEngString(fra), normalizeEngString(eng)] for fra, eng in zip(french, english)]\n",
    "\n",
    "# Filter sentence pairs, and deal with sentences that exceed MAX_LENGTH length\n",
    "input_lang = Lang('French')\n",
    "output_lang = Lang('English')\n",
    "pairs = [pair for pair in pairs if filterPair(pair)]\n",
    "print('Valid sentence pairs：', len(pairs))\n",
    "\n",
    "# Create two dictionaries (French and English)\n",
    "for pair in pairs:\n",
    "    input_lang.addSentence(pair[0])\n",
    "    output_lang.addSentence(pair[1])\n",
    "print(\"Total words:\")\n",
    "print(input_lang.name, input_lang.n_words)\n",
    "print(output_lang.name, output_lang.n_words)\n",
    "\n",
    "\n",
    "# Form the training set, first, disrupt the order of all sentences\n",
    "random_idx = np.random.permutation(range(len(pairs)))\n",
    "pairs = [pairs[i] for i in random_idx]\n",
    "\n",
    "# Sentence id formed by encoding the language into words\n",
    "pairs = [indexFromPair(pair) for pair in pairs]\n",
    "\n",
    "# Form training set, verification set and test set\n",
    "valid_size = len(pairs) // 10\n",
    "if valid_size > 10000:\n",
    "    valid_size = 10000\n",
    "\n",
    "valid_pairs = pairs[-valid_size : -valid_size // 2]\n",
    "test_pairs = pairs[- valid_size // 2 :]\n",
    "pairs = pairs[ : - valid_size]\n",
    "\n",
    "# Use PyTorch's dataset and dataloader objects to load the data into the loader and automatically batch\n",
    "\n",
    "batch_size = 512 \n",
    "\n",
    "print('Training   set samples：', len(pairs))\n",
    "print('Validation set samples：', len(valid_pairs))\n",
    "print('Test       set samples：', len(test_pairs))\n",
    "\n",
    "# Form a list of training pairs for feeding to train_dataset\n",
    "pairs_X = [pair[0] for pair in pairs]\n",
    "pairs_Y = [pair[1] for pair in pairs]\n",
    "valid_X = [pair[0] for pair in valid_pairs]\n",
    "valid_Y = [pair[1] for pair in valid_pairs]\n",
    "test_X = [pair[0] for pair in test_pairs]\n",
    "test_Y = [pair[1] for pair in test_pairs]\n",
    "\n",
    "\n",
    "# Training   set\n",
    "train_dataset = DataSet.TensorDataset(torch.LongTensor(pairs_X), torch.LongTensor(pairs_Y))\n",
    "train_loader = DataSet.DataLoader(train_dataset, batch_size = batch_size, shuffle = True, num_workers=8)\n",
    "\n",
    "# Validation set\n",
    "valid_dataset = DataSet.TensorDataset(torch.LongTensor(valid_X), torch.LongTensor(valid_Y))\n",
    "valid_loader = DataSet.DataLoader(valid_dataset, batch_size = batch_size, shuffle = True, num_workers=8)\n",
    "\n",
    "# Test       set\n",
    "test_dataset = DataSet.TensorDataset(torch.LongTensor(test_X), torch.LongTensor(test_Y))\n",
    "test_loader = DataSet.DataLoader(test_dataset, batch_size = batch_size, shuffle = True, num_workers = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T19:42:26.435426Z",
     "start_time": "2020-11-16T19:42:26.432263Z"
    }
   },
   "source": [
    "# 3. Encoder-decoder model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T19:42:39.067527Z",
     "start_time": "2020-11-16T19:42:39.064608Z"
    }
   },
   "source": [
    "## 3.1 Implementation of the encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T20:19:54.762155Z",
     "start_time": "2020-11-16T20:19:54.756011Z"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first = True, \n",
    "                          num_layers = self.n_layers, bidirectional = True)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        #input： batch_size, length_seq\n",
    "        embedded = self.embedding(input)\n",
    "        #embedded：batch_size, length_seq, hidden_size\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        # output：batch_size, length_seq, hidden_size\n",
    "        # hidden：num_layers * directions, batch_size, hidden_size\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        #num_layers * num_directions, batch, hidden_size\n",
    "        result = Variable(torch.zeros(self.n_layers * 2, batch_size, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T19:42:55.524167Z",
     "start_time": "2020-11-16T19:42:55.521145Z"
    }
   },
   "source": [
    "## 3.2 Implementation of the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T20:19:59.075985Z",
     "start_time": "2020-11-16T20:19:59.060966Z"
    }
   },
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, n_layers=1, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "\n",
    "        # from （hidden_size * (2 * layers+ 1) to   the maximum sequence length\n",
    "        self.attn = nn.Linear(self.hidden_size * (2 * n_layers + 1), self.max_length)\n",
    "\n",
    "        #The result of the attention mechanism       hidden_size * 3， project to      hidden_size\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 3, self.hidden_size)\n",
    "\n",
    "        # dropout\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "\n",
    "\n",
    "        # Define a bidirectional GRU, and is set to True to facilitate the operation batch_first\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size, bidirectional = True,\n",
    "                         num_layers = self.n_layers, batch_first = True)\n",
    "        self.out = nn.Linear(self.hidden_size * 2, self.output_size)\n",
    "        #hidden_size * 3， project to      output_size (Dictionary size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        # input：batch_size, length_seq\n",
    "        embedded = self.embedding(input)\n",
    "        # embedded：batch_size, length_seq, hidden_size\n",
    "        embedded = embedded[:, 0, :]\n",
    "        # embedded：batch_size, hidden_size  \n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "\n",
    "        # encoder_hidden：direction*n_layer, batch_size, hidden_size\n",
    "        # hidden：direction*n_layer, batch_size, hidden_size\n",
    "        temp_for_transpose = torch.transpose(hidden, 0, 1).contiguous()\n",
    "        # After transpose, permute and other dimensional transformation operations, tensor is no longer continuously stored in memory, \n",
    "        # and view operations require continuous storage in tensor memory, so contiguous is required to return a contiguous copy;\n",
    "        # hidden：batch_size, direction*n_layer, hidden_size\n",
    "        temp_for_transpose = temp_for_transpose.view(temp_for_transpose.size()[0], -1)\n",
    "        hidden_attn = temp_for_transpose\n",
    "        # hidden_attn：batch_size, direction*n_layers*hidden_size\n",
    "        input_to_attention = torch.cat((embedded, hidden_attn), 1)   \n",
    "        # input_to_attention：batch_size, hidden_size * (1 （The output of the last layer of encoder）+ direction * n_layers)\n",
    "\n",
    "        # The weight of the attention layer output\n",
    "        attn_weights = F.softmax(self.attn(input_to_attention))\n",
    "        # attn_weights：batch_size, max_length\n",
    "\n",
    "        # When the input data is not of equal length, cut the necessary section of weights. \n",
    "        # The sentence may not be long enough, so do not consider the weight of the padding part\n",
    "        # encoder_outputs：batch_size, length_seq, hidden_size*direction        encoder_outputs.size()[1]  =  length_seq\n",
    "        attn_weights = attn_weights[:, : encoder_outputs.size()[1]]\n",
    "        # attn_weights：batch_size, length_seq_of_encoder\n",
    "        attn_weights = attn_weights.unsqueeze(1)\n",
    "        # attn_weights：batch_size, 1, length_seq       1 for bmm\n",
    "\n",
    "        # encoder_outputs：batch_size, seq_length, hidden_size*direction\n",
    "        attn_applied = torch.bmm(attn_weights, encoder_outputs) \n",
    "        # attn_applied：batch_size, 1, hidden_size*direction\n",
    "\n",
    "        # concate the input word vector and the result of the attention mechanism into a large input vector\n",
    "        output = torch.cat((embedded, attn_applied[:,0,:]), 1)\n",
    "        # output大小：batch_size, hidden_size * (direction + 1)\n",
    "\n",
    "        output = self.attn_combine(output).unsqueeze(1)\n",
    "        # output：batch_size, 1, hidden_size\n",
    "        # output：batch_size, length_seq, hidden_size\n",
    "        output = F.relu(output)\n",
    "\n",
    "        output = self.dropout(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        # output：batch_size, length_seq, hidden_size * directions\n",
    "        # hidden：n_layers * directions, batch_size, hidden_size             \n",
    "\n",
    "        #Take the result of the last step of the GRU operation and feed it to the last layer of the fully linked layer\n",
    "        output = self.out(output[:, -1, :])     #input    batch_size,  hidden_size * 2   \n",
    "        # output：batch_size * output_size\n",
    "\n",
    "        # logsoftmax\n",
    "        output = F.log_softmax(output, dim = 1)    \n",
    "        # output：batch_size * output_size     The element is the probability value of all words in the dictionary at a certain moment\n",
    "        # hidden: n_layers * directions, batch_size, hidden_size     Latent variables of all layers and all directions at the last moment \n",
    "        # attn_weights：batch_size, 1, length_seq          1 for bmm\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self, batch_size):\n",
    "        # n_layers * directions, batch_size, hidden_size\n",
    "        result = Variable(torch.zeros(self.n_layers * 2, batch_size, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            return result.cuda()\n",
    "        else:\n",
    "            return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T19:43:20.883372Z",
     "start_time": "2020-11-16T19:43:20.880334Z"
    }
   },
   "source": [
    "## 3.3 Start training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T19:43:34.989292Z",
     "start_time": "2020-11-16T19:43:34.986300Z"
    }
   },
   "source": [
    "### 3.3.1 Instantiate encoder, decoder, define optimizer, loss function and other components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T20:21:18.987092Z",
     "start_time": "2020-11-16T20:21:18.901098Z"
    }
   },
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "max_length = MAX_LENGTH\n",
    "n_layers = 1\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size, n_layers = n_layers)\n",
    "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.5,\n",
    "                         max_length = max_length, n_layers = n_layers)\n",
    "\n",
    "if use_cuda:\n",
    "    encoder = encoder.cuda()\n",
    "    decoder = decoder.cuda()\n",
    "\n",
    "learning_rate = 0.001\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "teacher_forcing_ratio = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T19:43:52.121551Z",
     "start_time": "2020-11-16T19:43:52.118505Z"
    }
   },
   "source": [
    "### 3.3.2 Define the training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T20:21:20.198817Z",
     "start_time": "2020-11-16T20:21:20.189841Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_losses = []\n",
    "\n",
    "print_loss_total = 0\n",
    "\n",
    "print_loss_avg = 0\n",
    "\n",
    "def train_attention_model():\n",
    "    global plot_losses\n",
    "    global print_loss_total\n",
    "    global print_loss_avg\n",
    "    print_loss_total = 0\n",
    "\n",
    "\n",
    "    for data in train_loader:    \n",
    "        input_variable = Variable(data[0]).cuda() if use_cuda else Variable(data[0])\n",
    "        # input_variable：batch_size, length_seq      \n",
    "        target_variable = Variable(data[1]).cuda() if use_cuda else Variable(data[1])\n",
    "        # target_variable：batch_size, length_seq\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_hidden = encoder.initHidden(data[0].size()[0])\n",
    "\n",
    "        loss = 0\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "        # encoder_outputs：batch_size, length_seq, hidden_size*direction\n",
    "        # encoder_hidden：direction*n_layer, batch_size, hidden_size\n",
    "\n",
    "\n",
    "        decoder_input = Variable(torch.LongTensor([[SOS_token]] * target_variable.size()[0]))\n",
    "        # decoder_input：batch_size, length_seq\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "        # decoder_hidden：direction*n_layer, batch_size, hidden_size\n",
    "\n",
    "        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "        if use_teacher_forcing:\n",
    "\n",
    "            for di in range(MAX_LENGTH):\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs)\n",
    "\n",
    "                loss += criterion(decoder_output, target_variable[:, di])\n",
    "                decoder_input = target_variable[:, di].unsqueeze(1)  # Teacher forcing\n",
    "                # decoder_input：batch_size, length_seq\n",
    "        else:\n",
    "            for di in range(MAX_LENGTH):\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs)\n",
    "                #decoder_ouput：batch_size, output_size(vocab_size)\n",
    "                topv, topi = decoder_output.data.topk(1, dim = 1)\n",
    "                #topi ：batch_size, k\n",
    "                ni = topi[:, 0]\n",
    "\n",
    "                decoder_input = Variable(ni.unsqueeze(1))\n",
    "                # decoder_input：batch_size, length_seq\n",
    "                decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "                loss += criterion(decoder_output, target_variable[:, di])\n",
    "\n",
    "        loss.backward()\n",
    "        loss = loss.cpu() if use_cuda else loss\n",
    "        \n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        print_loss_total += loss.data.numpy()\n",
    "\n",
    "    print_loss_avg = print_loss_total / len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T19:44:08.214077Z",
     "start_time": "2020-11-16T19:44:08.210632Z"
    }
   },
   "source": [
    "### 3.3.3 Model verification function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T20:21:21.531989Z",
     "start_time": "2020-11-16T20:21:21.524938Z"
    }
   },
   "outputs": [],
   "source": [
    "valid_loss = 0\n",
    "rights = []\n",
    "\n",
    "def evaluation_attention_model():\n",
    "    global valid_loss\n",
    "    global rights\n",
    "    valid_loss = 0\n",
    "    rights = []\n",
    "\n",
    "\n",
    "    for data in valid_loader:\n",
    "        input_variable = Variable(data[0]).cuda() if use_cuda else Variable(data[0])\n",
    "        # input_variable：batch_size, length_seq\n",
    "        target_variable = Variable(data[1]).cuda() if use_cuda else Variable(data[1])\n",
    "        # target_variable：batch_size, length_seq\n",
    "\n",
    "        encoder_hidden = encoder.initHidden(data[0].size()[0])\n",
    "\n",
    "        loss = 0\n",
    "        encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "        # encoder_outputs：batch_size, length_seq, hidden_size*direction\n",
    "        # encoder_hidden：direction*n_layer, batch_size, hidden_size\n",
    "\n",
    "        decoder_input = Variable(torch.LongTensor([[SOS_token]] * target_variable.size()[0]))\n",
    "        # decoder_input：batch_size, length_seq\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "        # decoder_hidden：direction*n_layer, batch_size, hidden_size\n",
    "\n",
    "        for di in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            #decoder_ouput：batch_size, output_size(vocab_size)\n",
    "            topv, topi = decoder_output.data.topk(1, dim = 1)\n",
    "            #topi ：batch_size, k\n",
    "            ni = topi[:, 0]\n",
    "\n",
    "            decoder_input = Variable(ni.unsqueeze(1))\n",
    "            # decoder_input：batch_size, length_seq\n",
    "            decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "            right = rightness(decoder_output, target_variable[:, di])\n",
    "            rights.append(right)\n",
    "            loss += criterion(decoder_output, target_variable[:, di])\n",
    "        loss = loss.cpu() if use_cuda else loss\n",
    "        valid_loss += loss.data.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T19:56:34.997363Z",
     "start_time": "2020-11-16T19:56:34.994358Z"
    }
   },
   "source": [
    "### 3.3.4 Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T20:25:36.672090Z",
     "start_time": "2020-11-16T20:21:23.420829Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mk632\\Anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\ipykernel_launcher.py:56: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process：0% Train-loss：30.0765，Valid-loss：24.3053，Word Accuracy：51.45%\n",
      "Process：1% Train-loss：21.7781，Valid-loss：20.5314，Word Accuracy：56.37%\n",
      "Process：2% Train-loss：18.0869，Valid-loss：18.0186，Word Accuracy：60.31%\n",
      "Process：3% Train-loss：15.4811，Valid-loss：15.9831，Word Accuracy：62.94%\n",
      "Process：4% Train-loss：13.2895，Valid-loss：14.5060，Word Accuracy：65.23%\n",
      "Process：5% Train-loss：11.3065，Valid-loss：13.5635，Word Accuracy：67.32%\n",
      "Process：6% Train-loss：9.6616，Valid-loss：12.7669，Word Accuracy：68.35%\n",
      "Process：7% Train-loss：8.2814，Valid-loss：12.3477，Word Accuracy：69.52%\n",
      "Process：8% Train-loss：7.1224，Valid-loss：12.0246，Word Accuracy：70.77%\n",
      "Process：9% Train-loss：6.1938，Valid-loss：11.8832，Word Accuracy：71.05%\n",
      "Process：10% Train-loss：5.4108，Valid-loss：11.7413，Word Accuracy：71.79%\n",
      "Process：11% Train-loss：4.7758，Valid-loss：11.7512，Word Accuracy：71.89%\n",
      "Process：12% Train-loss：4.2358，Valid-loss：11.7439，Word Accuracy：72.37%\n",
      "Process：13% Train-loss：3.7895，Valid-loss：11.8882，Word Accuracy：72.23%\n",
      "Process：14% Train-loss：3.4132，Valid-loss：11.6908，Word Accuracy：72.77%\n",
      "Process：15% Train-loss：3.0656，Valid-loss：11.6855，Word Accuracy：73.04%\n",
      "Process：16% Train-loss：2.7664，Valid-loss：11.8759，Word Accuracy：73.19%\n",
      "Process：17% Train-loss：2.5133，Valid-loss：12.0438，Word Accuracy：73.21%\n",
      "Process：18% Train-loss：2.2849，Valid-loss：12.0957，Word Accuracy：73.14%\n",
      "Process：19% Train-loss：2.0989，Valid-loss：12.1584，Word Accuracy：73.24%\n",
      "Process：20% Train-loss：1.9258，Valid-loss：12.1921，Word Accuracy：73.19%\n",
      "Process：21% Train-loss：1.7627，Valid-loss：12.4063，Word Accuracy：73.21%\n",
      "Process：22% Train-loss：1.6262，Valid-loss：12.5631，Word Accuracy：73.13%\n",
      "Process：23% Train-loss：1.5115，Valid-loss：12.7669，Word Accuracy：73.48%\n",
      "Process：24% Train-loss：1.4092，Valid-loss：12.7504，Word Accuracy：73.65%\n",
      "Process：25% Train-loss：1.3038，Valid-loss：12.7606，Word Accuracy：73.28%\n",
      "Process：26% Train-loss：1.2313，Valid-loss：12.9950，Word Accuracy：73.35%\n",
      "Process：27% Train-loss：1.1613，Valid-loss：13.0694，Word Accuracy：73.65%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-144-65d972b866a4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mprint_loss_total\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# 调用训练函数\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mtrain_attention_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m# 将解码器置于测试状态，关闭dropout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-142-774559a48884>\u001b[0m in \u001b[0;36mtrain_attention_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# 对训练数据进行循环\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m#data为一个批次的很多句子 id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0minput_variable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;31m# input_variable的大小：batch_size, length_seq\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    972\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    973\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 974\u001b[1;33m             \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    975\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    939\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    940\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 941\u001b[1;33m                 \u001b[0msuccess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    942\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    943\u001b[0m                     \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    777\u001b[0m         \u001b[1;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch-gpu\\lib\\multiprocessing\\queues.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    102\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    105\u001b[0m                         \u001b[1;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch-gpu\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36mpoll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch-gpu\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36m_poll\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    328\u001b[0m                         _winapi.PeekNamedPipe(self._handle)[0] != 0):\n\u001b[0;32m    329\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0m_get_more_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mov\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxsize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch-gpu\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(object_list, timeout)\u001b[0m\n\u001b[0;32m    867\u001b[0m                         \u001b[0mtimeout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 869\u001b[1;33m             \u001b[0mready_handles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_exhaustive_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwaithandle_to_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    870\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m             \u001b[1;31m# request that overlapped reads stop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\pytorch-gpu\\lib\\multiprocessing\\connection.py\u001b[0m in \u001b[0;36m_exhaustive_wait\u001b[1;34m(handles, timeout)\u001b[0m\n\u001b[0;32m    799\u001b[0m         \u001b[0mready\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    800\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 801\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_winapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWaitForMultipleObjects\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    802\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mWAIT_TIMEOUT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    803\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epoch = 100\n",
    "\n",
    "plot_losses = []\n",
    "for epoch in range(num_epoch):\n",
    "    decoder.train()\n",
    "    \n",
    "    learning_rate = learning_rate -epoch * 0.00001\n",
    "    #reduce teacher_forcing_ratio gruadually\n",
    "    teacher_forcing_ratio = teacher_forcing_ratio - epoch * 0.05\n",
    "\n",
    "    print_loss_total = 0\n",
    "\n",
    "    train_attention_model()\n",
    "\n",
    "    # close dropout\n",
    "    decoder.eval()\n",
    "\n",
    "    evaluation_attention_model()\n",
    "      \n",
    "    right_ratio = 1.0 * np.sum([i[0] for i in rights]) / np.sum([i[1] for i in rights])\n",
    "    print('Process：%d%% Train-loss：%.4f，Valid-loss：%.4f，Word Accuracy：%.2f%%' % (epoch * 1.0 / num_epoch * 100, \n",
    "                                                    print_loss_avg,\n",
    "                                                    valid_loss / len(valid_loader),\n",
    "                                                    100.0 * right_ratio))\n",
    "    plot_losses.append([print_loss_avg, valid_loss / len(valid_loader), right_ratio])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T21:34:18.325151Z",
     "start_time": "2020-11-16T21:34:18.321420Z"
    }
   },
   "source": [
    "### 3.3.5 Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T20:26:11.233889Z",
     "start_time": "2020-11-16T20:26:11.116873Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(encoder,'encoder_attention2.mdl')\n",
    "torch.save(decoder,'decoder_attention2.mdl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T21:34:36.877621Z",
     "start_time": "2020-11-16T21:34:36.874571Z"
    }
   },
   "source": [
    "### 3.3.6 Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T20:26:29.503366Z",
     "start_time": "2020-11-16T20:26:29.453088Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder = torch.load(\"encoder_attention2.mdl\")\n",
    "decoder = torch.load(\"decoder_attention2.mdl\")\n",
    "\n",
    "if use_cuda:\n",
    "    encoder = encoder.cuda()\n",
    "    decoder = decoder.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T21:33:53.188561Z",
     "start_time": "2020-11-16T21:33:53.186012Z"
    }
   },
   "source": [
    "### 3.3.7 Drawing the training error curve of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T20:27:05.619054Z",
     "start_time": "2020-11-16T20:27:05.573945Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread QueueFeederThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mk632\\Anaconda3\\envs\\pytorch-gpu\\lib\\multiprocessing\\queues.py\", line 232, in _feed\n",
      "    close()\n",
      "  File \"C:\\Users\\mk632\\Anaconda3\\envs\\pytorch-gpu\\lib\\multiprocessing\\connection.py\", line 177, in close\n",
      "    self._close()\n",
      "  File \"C:\\Users\\mk632\\Anaconda3\\envs\\pytorch-gpu\\lib\\multiprocessing\\connection.py\", line 277, in _close\n",
      "    _CloseHandle(self._handle)\n",
      "OSError: [WinError 6] 句柄无效。\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\mk632\\Anaconda3\\envs\\pytorch-gpu\\lib\\threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"C:\\Users\\mk632\\Anaconda3\\envs\\pytorch-gpu\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"C:\\Users\\mk632\\Anaconda3\\envs\\pytorch-gpu\\lib\\multiprocessing\\queues.py\", line 263, in _feed\n",
      "    queue_sem.release()\n",
      "ValueError: semaphore or lock released too many times\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "/* Put everything inside the global mpl namespace */\n",
       "/* global mpl */\n",
       "window.mpl = {};\n",
       "\n",
       "mpl.get_websocket_type = function () {\n",
       "    if (typeof WebSocket !== 'undefined') {\n",
       "        return WebSocket;\n",
       "    } else if (typeof MozWebSocket !== 'undefined') {\n",
       "        return MozWebSocket;\n",
       "    } else {\n",
       "        alert(\n",
       "            'Your browser does not have WebSocket support. ' +\n",
       "                'Please try Chrome, Safari or Firefox ≥ 6. ' +\n",
       "                'Firefox 4 and 5 are also supported but you ' +\n",
       "                'have to enable WebSockets in about:config.'\n",
       "        );\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure = function (figure_id, websocket, ondownload, parent_element) {\n",
       "    this.id = figure_id;\n",
       "\n",
       "    this.ws = websocket;\n",
       "\n",
       "    this.supports_binary = this.ws.binaryType !== undefined;\n",
       "\n",
       "    if (!this.supports_binary) {\n",
       "        var warnings = document.getElementById('mpl-warnings');\n",
       "        if (warnings) {\n",
       "            warnings.style.display = 'block';\n",
       "            warnings.textContent =\n",
       "                'This browser does not support binary websocket messages. ' +\n",
       "                'Performance may be slow.';\n",
       "        }\n",
       "    }\n",
       "\n",
       "    this.imageObj = new Image();\n",
       "\n",
       "    this.context = undefined;\n",
       "    this.message = undefined;\n",
       "    this.canvas = undefined;\n",
       "    this.rubberband_canvas = undefined;\n",
       "    this.rubberband_context = undefined;\n",
       "    this.format_dropdown = undefined;\n",
       "\n",
       "    this.image_mode = 'full';\n",
       "\n",
       "    this.root = document.createElement('div');\n",
       "    this.root.setAttribute('style', 'display: inline-block');\n",
       "    this._root_extra_style(this.root);\n",
       "\n",
       "    parent_element.appendChild(this.root);\n",
       "\n",
       "    this._init_header(this);\n",
       "    this._init_canvas(this);\n",
       "    this._init_toolbar(this);\n",
       "\n",
       "    var fig = this;\n",
       "\n",
       "    this.waiting = false;\n",
       "\n",
       "    this.ws.onopen = function () {\n",
       "        fig.send_message('supports_binary', { value: fig.supports_binary });\n",
       "        fig.send_message('send_image_mode', {});\n",
       "        if (fig.ratio !== 1) {\n",
       "            fig.send_message('set_dpi_ratio', { dpi_ratio: fig.ratio });\n",
       "        }\n",
       "        fig.send_message('refresh', {});\n",
       "    };\n",
       "\n",
       "    this.imageObj.onload = function () {\n",
       "        if (fig.image_mode === 'full') {\n",
       "            // Full images could contain transparency (where diff images\n",
       "            // almost always do), so we need to clear the canvas so that\n",
       "            // there is no ghosting.\n",
       "            fig.context.clearRect(0, 0, fig.canvas.width, fig.canvas.height);\n",
       "        }\n",
       "        fig.context.drawImage(fig.imageObj, 0, 0);\n",
       "    };\n",
       "\n",
       "    this.imageObj.onunload = function () {\n",
       "        fig.ws.close();\n",
       "    };\n",
       "\n",
       "    this.ws.onmessage = this._make_on_message_function(this);\n",
       "\n",
       "    this.ondownload = ondownload;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_header = function () {\n",
       "    var titlebar = document.createElement('div');\n",
       "    titlebar.classList =\n",
       "        'ui-dialog-titlebar ui-widget-header ui-corner-all ui-helper-clearfix';\n",
       "    var titletext = document.createElement('div');\n",
       "    titletext.classList = 'ui-dialog-title';\n",
       "    titletext.setAttribute(\n",
       "        'style',\n",
       "        'width: 100%; text-align: center; padding: 3px;'\n",
       "    );\n",
       "    titlebar.appendChild(titletext);\n",
       "    this.root.appendChild(titlebar);\n",
       "    this.header = titletext;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function (_canvas_div) {};\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function (_canvas_div) {};\n",
       "\n",
       "mpl.figure.prototype._init_canvas = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var canvas_div = (this.canvas_div = document.createElement('div'));\n",
       "    canvas_div.setAttribute(\n",
       "        'style',\n",
       "        'border: 1px solid #ddd;' +\n",
       "            'box-sizing: content-box;' +\n",
       "            'clear: both;' +\n",
       "            'min-height: 1px;' +\n",
       "            'min-width: 1px;' +\n",
       "            'outline: 0;' +\n",
       "            'overflow: hidden;' +\n",
       "            'position: relative;' +\n",
       "            'resize: both;'\n",
       "    );\n",
       "\n",
       "    function on_keyboard_event_closure(name) {\n",
       "        return function (event) {\n",
       "            return fig.key_event(event, name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    canvas_div.addEventListener(\n",
       "        'keydown',\n",
       "        on_keyboard_event_closure('key_press')\n",
       "    );\n",
       "    canvas_div.addEventListener(\n",
       "        'keyup',\n",
       "        on_keyboard_event_closure('key_release')\n",
       "    );\n",
       "\n",
       "    this._canvas_extra_style(canvas_div);\n",
       "    this.root.appendChild(canvas_div);\n",
       "\n",
       "    var canvas = (this.canvas = document.createElement('canvas'));\n",
       "    canvas.classList.add('mpl-canvas');\n",
       "    canvas.setAttribute('style', 'box-sizing: content-box;');\n",
       "\n",
       "    this.context = canvas.getContext('2d');\n",
       "\n",
       "    var backingStore =\n",
       "        this.context.backingStorePixelRatio ||\n",
       "        this.context.webkitBackingStorePixelRatio ||\n",
       "        this.context.mozBackingStorePixelRatio ||\n",
       "        this.context.msBackingStorePixelRatio ||\n",
       "        this.context.oBackingStorePixelRatio ||\n",
       "        this.context.backingStorePixelRatio ||\n",
       "        1;\n",
       "\n",
       "    this.ratio = (window.devicePixelRatio || 1) / backingStore;\n",
       "    if (this.ratio !== 1) {\n",
       "        fig.send_message('set_dpi_ratio', { dpi_ratio: this.ratio });\n",
       "    }\n",
       "\n",
       "    var rubberband_canvas = (this.rubberband_canvas = document.createElement(\n",
       "        'canvas'\n",
       "    ));\n",
       "    rubberband_canvas.setAttribute(\n",
       "        'style',\n",
       "        'box-sizing: content-box; position: absolute; left: 0; top: 0; z-index: 1;'\n",
       "    );\n",
       "\n",
       "    var resizeObserver = new ResizeObserver(function (entries) {\n",
       "        var nentries = entries.length;\n",
       "        for (var i = 0; i < nentries; i++) {\n",
       "            var entry = entries[i];\n",
       "            var width, height;\n",
       "            if (entry.contentBoxSize) {\n",
       "                if (entry.contentBoxSize instanceof Array) {\n",
       "                    // Chrome 84 implements new version of spec.\n",
       "                    width = entry.contentBoxSize[0].inlineSize;\n",
       "                    height = entry.contentBoxSize[0].blockSize;\n",
       "                } else {\n",
       "                    // Firefox implements old version of spec.\n",
       "                    width = entry.contentBoxSize.inlineSize;\n",
       "                    height = entry.contentBoxSize.blockSize;\n",
       "                }\n",
       "            } else {\n",
       "                // Chrome <84 implements even older version of spec.\n",
       "                width = entry.contentRect.width;\n",
       "                height = entry.contentRect.height;\n",
       "            }\n",
       "\n",
       "            // Keep the size of the canvas and rubber band canvas in sync with\n",
       "            // the canvas container.\n",
       "            if (entry.devicePixelContentBoxSize) {\n",
       "                // Chrome 84 implements new version of spec.\n",
       "                canvas.setAttribute(\n",
       "                    'width',\n",
       "                    entry.devicePixelContentBoxSize[0].inlineSize\n",
       "                );\n",
       "                canvas.setAttribute(\n",
       "                    'height',\n",
       "                    entry.devicePixelContentBoxSize[0].blockSize\n",
       "                );\n",
       "            } else {\n",
       "                canvas.setAttribute('width', width * fig.ratio);\n",
       "                canvas.setAttribute('height', height * fig.ratio);\n",
       "            }\n",
       "            canvas.setAttribute(\n",
       "                'style',\n",
       "                'width: ' + width + 'px; height: ' + height + 'px;'\n",
       "            );\n",
       "\n",
       "            rubberband_canvas.setAttribute('width', width);\n",
       "            rubberband_canvas.setAttribute('height', height);\n",
       "\n",
       "            // And update the size in Python. We ignore the initial 0/0 size\n",
       "            // that occurs as the element is placed into the DOM, which should\n",
       "            // otherwise not happen due to the minimum size styling.\n",
       "            if (width != 0 && height != 0) {\n",
       "                fig.request_resize(width, height);\n",
       "            }\n",
       "        }\n",
       "    });\n",
       "    resizeObserver.observe(canvas_div);\n",
       "\n",
       "    function on_mouse_event_closure(name) {\n",
       "        return function (event) {\n",
       "            return fig.mouse_event(event, name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mousedown',\n",
       "        on_mouse_event_closure('button_press')\n",
       "    );\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mouseup',\n",
       "        on_mouse_event_closure('button_release')\n",
       "    );\n",
       "    // Throttle sequential mouse events to 1 every 20ms.\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mousemove',\n",
       "        on_mouse_event_closure('motion_notify')\n",
       "    );\n",
       "\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mouseenter',\n",
       "        on_mouse_event_closure('figure_enter')\n",
       "    );\n",
       "    rubberband_canvas.addEventListener(\n",
       "        'mouseleave',\n",
       "        on_mouse_event_closure('figure_leave')\n",
       "    );\n",
       "\n",
       "    canvas_div.addEventListener('wheel', function (event) {\n",
       "        if (event.deltaY < 0) {\n",
       "            event.step = 1;\n",
       "        } else {\n",
       "            event.step = -1;\n",
       "        }\n",
       "        on_mouse_event_closure('scroll')(event);\n",
       "    });\n",
       "\n",
       "    canvas_div.appendChild(canvas);\n",
       "    canvas_div.appendChild(rubberband_canvas);\n",
       "\n",
       "    this.rubberband_context = rubberband_canvas.getContext('2d');\n",
       "    this.rubberband_context.strokeStyle = '#000000';\n",
       "\n",
       "    this._resize_canvas = function (width, height, forward) {\n",
       "        if (forward) {\n",
       "            canvas_div.style.width = width + 'px';\n",
       "            canvas_div.style.height = height + 'px';\n",
       "        }\n",
       "    };\n",
       "\n",
       "    // Disable right mouse context menu.\n",
       "    this.rubberband_canvas.addEventListener('contextmenu', function (_e) {\n",
       "        event.preventDefault();\n",
       "        return false;\n",
       "    });\n",
       "\n",
       "    function set_focus() {\n",
       "        canvas.focus();\n",
       "        canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    window.setTimeout(set_focus, 100);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var toolbar = document.createElement('div');\n",
       "    toolbar.classList = 'mpl-toolbar';\n",
       "    this.root.appendChild(toolbar);\n",
       "\n",
       "    function on_click_closure(name) {\n",
       "        return function (_event) {\n",
       "            return fig.toolbar_button_onclick(name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    function on_mouseover_closure(tooltip) {\n",
       "        return function (event) {\n",
       "            if (!event.currentTarget.disabled) {\n",
       "                return fig.toolbar_button_onmouseover(tooltip);\n",
       "            }\n",
       "        };\n",
       "    }\n",
       "\n",
       "    fig.buttons = {};\n",
       "    var buttonGroup = document.createElement('div');\n",
       "    buttonGroup.classList = 'mpl-button-group';\n",
       "    for (var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            /* Instead of a spacer, we start a new button group. */\n",
       "            if (buttonGroup.hasChildNodes()) {\n",
       "                toolbar.appendChild(buttonGroup);\n",
       "            }\n",
       "            buttonGroup = document.createElement('div');\n",
       "            buttonGroup.classList = 'mpl-button-group';\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        var button = (fig.buttons[name] = document.createElement('button'));\n",
       "        button.classList = 'mpl-widget';\n",
       "        button.setAttribute('role', 'button');\n",
       "        button.setAttribute('aria-disabled', 'false');\n",
       "        button.addEventListener('click', on_click_closure(method_name));\n",
       "        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n",
       "\n",
       "        var icon_img = document.createElement('img');\n",
       "        icon_img.src = '_images/' + image + '.png';\n",
       "        icon_img.srcset = '_images/' + image + '_large.png 2x';\n",
       "        icon_img.alt = tooltip;\n",
       "        button.appendChild(icon_img);\n",
       "\n",
       "        buttonGroup.appendChild(button);\n",
       "    }\n",
       "\n",
       "    if (buttonGroup.hasChildNodes()) {\n",
       "        toolbar.appendChild(buttonGroup);\n",
       "    }\n",
       "\n",
       "    var fmt_picker = document.createElement('select');\n",
       "    fmt_picker.classList = 'mpl-widget';\n",
       "    toolbar.appendChild(fmt_picker);\n",
       "    this.format_dropdown = fmt_picker;\n",
       "\n",
       "    for (var ind in mpl.extensions) {\n",
       "        var fmt = mpl.extensions[ind];\n",
       "        var option = document.createElement('option');\n",
       "        option.selected = fmt === mpl.default_extension;\n",
       "        option.innerHTML = fmt;\n",
       "        fmt_picker.appendChild(option);\n",
       "    }\n",
       "\n",
       "    var status_bar = document.createElement('span');\n",
       "    status_bar.classList = 'mpl-message';\n",
       "    toolbar.appendChild(status_bar);\n",
       "    this.message = status_bar;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.request_resize = function (x_pixels, y_pixels) {\n",
       "    // Request matplotlib to resize the figure. Matplotlib will then trigger a resize in the client,\n",
       "    // which will in turn request a refresh of the image.\n",
       "    this.send_message('resize', { width: x_pixels, height: y_pixels });\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.send_message = function (type, properties) {\n",
       "    properties['type'] = type;\n",
       "    properties['figure_id'] = this.id;\n",
       "    this.ws.send(JSON.stringify(properties));\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.send_draw_message = function () {\n",
       "    if (!this.waiting) {\n",
       "        this.waiting = true;\n",
       "        this.ws.send(JSON.stringify({ type: 'draw', figure_id: this.id }));\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_save = function (fig, _msg) {\n",
       "    var format_dropdown = fig.format_dropdown;\n",
       "    var format = format_dropdown.options[format_dropdown.selectedIndex].value;\n",
       "    fig.ondownload(fig, format);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_resize = function (fig, msg) {\n",
       "    var size = msg['size'];\n",
       "    if (size[0] !== fig.canvas.width || size[1] !== fig.canvas.height) {\n",
       "        fig._resize_canvas(size[0], size[1], msg['forward']);\n",
       "        fig.send_message('refresh', {});\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_rubberband = function (fig, msg) {\n",
       "    var x0 = msg['x0'] / fig.ratio;\n",
       "    var y0 = (fig.canvas.height - msg['y0']) / fig.ratio;\n",
       "    var x1 = msg['x1'] / fig.ratio;\n",
       "    var y1 = (fig.canvas.height - msg['y1']) / fig.ratio;\n",
       "    x0 = Math.floor(x0) + 0.5;\n",
       "    y0 = Math.floor(y0) + 0.5;\n",
       "    x1 = Math.floor(x1) + 0.5;\n",
       "    y1 = Math.floor(y1) + 0.5;\n",
       "    var min_x = Math.min(x0, x1);\n",
       "    var min_y = Math.min(y0, y1);\n",
       "    var width = Math.abs(x1 - x0);\n",
       "    var height = Math.abs(y1 - y0);\n",
       "\n",
       "    fig.rubberband_context.clearRect(\n",
       "        0,\n",
       "        0,\n",
       "        fig.canvas.width / fig.ratio,\n",
       "        fig.canvas.height / fig.ratio\n",
       "    );\n",
       "\n",
       "    fig.rubberband_context.strokeRect(min_x, min_y, width, height);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_figure_label = function (fig, msg) {\n",
       "    // Updates the figure title.\n",
       "    fig.header.textContent = msg['label'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_cursor = function (fig, msg) {\n",
       "    var cursor = msg['cursor'];\n",
       "    switch (cursor) {\n",
       "        case 0:\n",
       "            cursor = 'pointer';\n",
       "            break;\n",
       "        case 1:\n",
       "            cursor = 'default';\n",
       "            break;\n",
       "        case 2:\n",
       "            cursor = 'crosshair';\n",
       "            break;\n",
       "        case 3:\n",
       "            cursor = 'move';\n",
       "            break;\n",
       "    }\n",
       "    fig.rubberband_canvas.style.cursor = cursor;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_message = function (fig, msg) {\n",
       "    fig.message.textContent = msg['message'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_draw = function (fig, _msg) {\n",
       "    // Request the server to send over a new figure.\n",
       "    fig.send_draw_message();\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_image_mode = function (fig, msg) {\n",
       "    fig.image_mode = msg['mode'];\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_history_buttons = function (fig, msg) {\n",
       "    for (var key in msg) {\n",
       "        if (!(key in fig.buttons)) {\n",
       "            continue;\n",
       "        }\n",
       "        fig.buttons[key].disabled = !msg[key];\n",
       "        fig.buttons[key].setAttribute('aria-disabled', !msg[key]);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_navigate_mode = function (fig, msg) {\n",
       "    if (msg['mode'] === 'PAN') {\n",
       "        fig.buttons['Pan'].classList.add('active');\n",
       "        fig.buttons['Zoom'].classList.remove('active');\n",
       "    } else if (msg['mode'] === 'ZOOM') {\n",
       "        fig.buttons['Pan'].classList.remove('active');\n",
       "        fig.buttons['Zoom'].classList.add('active');\n",
       "    } else {\n",
       "        fig.buttons['Pan'].classList.remove('active');\n",
       "        fig.buttons['Zoom'].classList.remove('active');\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function () {\n",
       "    // Called whenever the canvas gets updated.\n",
       "    this.send_message('ack', {});\n",
       "};\n",
       "\n",
       "// A function to construct a web socket function for onmessage handling.\n",
       "// Called in the figure constructor.\n",
       "mpl.figure.prototype._make_on_message_function = function (fig) {\n",
       "    return function socket_on_message(evt) {\n",
       "        if (evt.data instanceof Blob) {\n",
       "            /* FIXME: We get \"Resource interpreted as Image but\n",
       "             * transferred with MIME type text/plain:\" errors on\n",
       "             * Chrome.  But how to set the MIME type?  It doesn't seem\n",
       "             * to be part of the websocket stream */\n",
       "            evt.data.type = 'image/png';\n",
       "\n",
       "            /* Free the memory for the previous frames */\n",
       "            if (fig.imageObj.src) {\n",
       "                (window.URL || window.webkitURL).revokeObjectURL(\n",
       "                    fig.imageObj.src\n",
       "                );\n",
       "            }\n",
       "\n",
       "            fig.imageObj.src = (window.URL || window.webkitURL).createObjectURL(\n",
       "                evt.data\n",
       "            );\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        } else if (\n",
       "            typeof evt.data === 'string' &&\n",
       "            evt.data.slice(0, 21) === 'data:image/png;base64'\n",
       "        ) {\n",
       "            fig.imageObj.src = evt.data;\n",
       "            fig.updated_canvas_event();\n",
       "            fig.waiting = false;\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        var msg = JSON.parse(evt.data);\n",
       "        var msg_type = msg['type'];\n",
       "\n",
       "        // Call the  \"handle_{type}\" callback, which takes\n",
       "        // the figure and JSON message as its only arguments.\n",
       "        try {\n",
       "            var callback = fig['handle_' + msg_type];\n",
       "        } catch (e) {\n",
       "            console.log(\n",
       "                \"No handler for the '\" + msg_type + \"' message type: \",\n",
       "                msg\n",
       "            );\n",
       "            return;\n",
       "        }\n",
       "\n",
       "        if (callback) {\n",
       "            try {\n",
       "                // console.log(\"Handling '\" + msg_type + \"' message: \", msg);\n",
       "                callback(fig, msg);\n",
       "            } catch (e) {\n",
       "                console.log(\n",
       "                    \"Exception inside the 'handler_\" + msg_type + \"' callback:\",\n",
       "                    e,\n",
       "                    e.stack,\n",
       "                    msg\n",
       "                );\n",
       "            }\n",
       "        }\n",
       "    };\n",
       "};\n",
       "\n",
       "// from http://stackoverflow.com/questions/1114465/getting-mouse-location-in-canvas\n",
       "mpl.findpos = function (e) {\n",
       "    //this section is from http://www.quirksmode.org/js/events_properties.html\n",
       "    var targ;\n",
       "    if (!e) {\n",
       "        e = window.event;\n",
       "    }\n",
       "    if (e.target) {\n",
       "        targ = e.target;\n",
       "    } else if (e.srcElement) {\n",
       "        targ = e.srcElement;\n",
       "    }\n",
       "    if (targ.nodeType === 3) {\n",
       "        // defeat Safari bug\n",
       "        targ = targ.parentNode;\n",
       "    }\n",
       "\n",
       "    // pageX,Y are the mouse positions relative to the document\n",
       "    var boundingRect = targ.getBoundingClientRect();\n",
       "    var x = e.pageX - (boundingRect.left + document.body.scrollLeft);\n",
       "    var y = e.pageY - (boundingRect.top + document.body.scrollTop);\n",
       "\n",
       "    return { x: x, y: y };\n",
       "};\n",
       "\n",
       "/*\n",
       " * return a copy of an object with only non-object keys\n",
       " * we need this to avoid circular references\n",
       " * http://stackoverflow.com/a/24161582/3208463\n",
       " */\n",
       "function simpleKeys(original) {\n",
       "    return Object.keys(original).reduce(function (obj, key) {\n",
       "        if (typeof original[key] !== 'object') {\n",
       "            obj[key] = original[key];\n",
       "        }\n",
       "        return obj;\n",
       "    }, {});\n",
       "}\n",
       "\n",
       "mpl.figure.prototype.mouse_event = function (event, name) {\n",
       "    var canvas_pos = mpl.findpos(event);\n",
       "\n",
       "    if (name === 'button_press') {\n",
       "        this.canvas.focus();\n",
       "        this.canvas_div.focus();\n",
       "    }\n",
       "\n",
       "    var x = canvas_pos.x * this.ratio;\n",
       "    var y = canvas_pos.y * this.ratio;\n",
       "\n",
       "    this.send_message(name, {\n",
       "        x: x,\n",
       "        y: y,\n",
       "        button: event.button,\n",
       "        step: event.step,\n",
       "        guiEvent: simpleKeys(event),\n",
       "    });\n",
       "\n",
       "    /* This prevents the web browser from automatically changing to\n",
       "     * the text insertion cursor when the button is pressed.  We want\n",
       "     * to control all of the cursor setting manually through the\n",
       "     * 'cursor' event from matplotlib */\n",
       "    event.preventDefault();\n",
       "    return false;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function (_event, _name) {\n",
       "    // Handle any extra behaviour associated with a key event\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.key_event = function (event, name) {\n",
       "    // Prevent repeat events\n",
       "    if (name === 'key_press') {\n",
       "        if (event.which === this._key) {\n",
       "            return;\n",
       "        } else {\n",
       "            this._key = event.which;\n",
       "        }\n",
       "    }\n",
       "    if (name === 'key_release') {\n",
       "        this._key = null;\n",
       "    }\n",
       "\n",
       "    var value = '';\n",
       "    if (event.ctrlKey && event.which !== 17) {\n",
       "        value += 'ctrl+';\n",
       "    }\n",
       "    if (event.altKey && event.which !== 18) {\n",
       "        value += 'alt+';\n",
       "    }\n",
       "    if (event.shiftKey && event.which !== 16) {\n",
       "        value += 'shift+';\n",
       "    }\n",
       "\n",
       "    value += 'k';\n",
       "    value += event.which.toString();\n",
       "\n",
       "    this._key_event_extra(event, name);\n",
       "\n",
       "    this.send_message(name, { key: value, guiEvent: simpleKeys(event) });\n",
       "    return false;\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onclick = function (name) {\n",
       "    if (name === 'download') {\n",
       "        this.handle_save(this, null);\n",
       "    } else {\n",
       "        this.send_message('toolbar_button', { name: name });\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.toolbar_button_onmouseover = function (tooltip) {\n",
       "    this.message.textContent = tooltip;\n",
       "};\n",
       "mpl.toolbar_items = [[\"Home\", \"Reset original view\", \"fa fa-home icon-home\", \"home\"], [\"Back\", \"Back to previous view\", \"fa fa-arrow-left icon-arrow-left\", \"back\"], [\"Forward\", \"Forward to next view\", \"fa fa-arrow-right icon-arrow-right\", \"forward\"], [\"\", \"\", \"\", \"\"], [\"Pan\", \"Left button pans, Right button zooms\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-arrows icon-move\", \"pan\"], [\"Zoom\", \"Zoom to rectangle\\nx/y fixes axis, CTRL fixes aspect\", \"fa fa-square-o icon-check-empty\", \"zoom\"], [\"\", \"\", \"\", \"\"], [\"Download\", \"Download plot\", \"fa fa-floppy-o icon-save\", \"download\"]];\n",
       "\n",
       "mpl.extensions = [\"eps\", \"jpeg\", \"pdf\", \"png\", \"ps\", \"raw\", \"svg\", \"tif\"];\n",
       "\n",
       "mpl.default_extension = \"png\";/* global mpl */\n",
       "\n",
       "var comm_websocket_adapter = function (comm) {\n",
       "    // Create a \"websocket\"-like object which calls the given IPython comm\n",
       "    // object with the appropriate methods. Currently this is a non binary\n",
       "    // socket, so there is still some room for performance tuning.\n",
       "    var ws = {};\n",
       "\n",
       "    ws.close = function () {\n",
       "        comm.close();\n",
       "    };\n",
       "    ws.send = function (m) {\n",
       "        //console.log('sending', m);\n",
       "        comm.send(m);\n",
       "    };\n",
       "    // Register the callback with on_msg.\n",
       "    comm.on_msg(function (msg) {\n",
       "        //console.log('receiving', msg['content']['data'], msg);\n",
       "        // Pass the mpl event to the overridden (by mpl) onmessage function.\n",
       "        ws.onmessage(msg['content']['data']);\n",
       "    });\n",
       "    return ws;\n",
       "};\n",
       "\n",
       "mpl.mpl_figure_comm = function (comm, msg) {\n",
       "    // This is the function which gets called when the mpl process\n",
       "    // starts-up an IPython Comm through the \"matplotlib\" channel.\n",
       "\n",
       "    var id = msg.content.data.id;\n",
       "    // Get hold of the div created by the display call when the Comm\n",
       "    // socket was opened in Python.\n",
       "    var element = document.getElementById(id);\n",
       "    var ws_proxy = comm_websocket_adapter(comm);\n",
       "\n",
       "    function ondownload(figure, _format) {\n",
       "        window.open(figure.canvas.toDataURL());\n",
       "    }\n",
       "\n",
       "    var fig = new mpl.figure(id, ws_proxy, ondownload, element);\n",
       "\n",
       "    // Call onopen now - mpl needs it, as it is assuming we've passed it a real\n",
       "    // web socket which is closed, not our websocket->open comm proxy.\n",
       "    ws_proxy.onopen();\n",
       "\n",
       "    fig.parent_element = element;\n",
       "    fig.cell_info = mpl.find_output_cell(\"<div id='\" + id + \"'></div>\");\n",
       "    if (!fig.cell_info) {\n",
       "        console.error('Failed to find cell for figure', id, fig);\n",
       "        return;\n",
       "    }\n",
       "    fig.cell_info[0].output_area.element.one(\n",
       "        'cleared',\n",
       "        { fig: fig },\n",
       "        fig._remove_fig_handler\n",
       "    );\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_close = function (fig, msg) {\n",
       "    var width = fig.canvas.width / fig.ratio;\n",
       "    fig.cell_info[0].output_area.element.off(\n",
       "        'cleared',\n",
       "        fig._remove_fig_handler\n",
       "    );\n",
       "\n",
       "    // Update the output cell to use the data from the current canvas.\n",
       "    fig.push_to_output();\n",
       "    var dataURL = fig.canvas.toDataURL();\n",
       "    // Re-enable the keyboard manager in IPython - without this line, in FF,\n",
       "    // the notebook keyboard shortcuts fail.\n",
       "    IPython.keyboard_manager.enable();\n",
       "    fig.parent_element.innerHTML =\n",
       "        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "    fig.close_ws(fig, msg);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.close_ws = function (fig, msg) {\n",
       "    fig.send_message('closing', msg);\n",
       "    // fig.ws.close()\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.push_to_output = function (_remove_interactive) {\n",
       "    // Turn the data on the canvas into data in the output cell.\n",
       "    var width = this.canvas.width / this.ratio;\n",
       "    var dataURL = this.canvas.toDataURL();\n",
       "    this.cell_info[1]['text/html'] =\n",
       "        '<img src=\"' + dataURL + '\" width=\"' + width + '\">';\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.updated_canvas_event = function () {\n",
       "    // Tell IPython that the notebook contents must change.\n",
       "    IPython.notebook.set_dirty(true);\n",
       "    this.send_message('ack', {});\n",
       "    var fig = this;\n",
       "    // Wait a second, then push the new image to the DOM so\n",
       "    // that it is saved nicely (might be nice to debounce this).\n",
       "    setTimeout(function () {\n",
       "        fig.push_to_output();\n",
       "    }, 1000);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._init_toolbar = function () {\n",
       "    var fig = this;\n",
       "\n",
       "    var toolbar = document.createElement('div');\n",
       "    toolbar.classList = 'btn-toolbar';\n",
       "    this.root.appendChild(toolbar);\n",
       "\n",
       "    function on_click_closure(name) {\n",
       "        return function (_event) {\n",
       "            return fig.toolbar_button_onclick(name);\n",
       "        };\n",
       "    }\n",
       "\n",
       "    function on_mouseover_closure(tooltip) {\n",
       "        return function (event) {\n",
       "            if (!event.currentTarget.disabled) {\n",
       "                return fig.toolbar_button_onmouseover(tooltip);\n",
       "            }\n",
       "        };\n",
       "    }\n",
       "\n",
       "    fig.buttons = {};\n",
       "    var buttonGroup = document.createElement('div');\n",
       "    buttonGroup.classList = 'btn-group';\n",
       "    var button;\n",
       "    for (var toolbar_ind in mpl.toolbar_items) {\n",
       "        var name = mpl.toolbar_items[toolbar_ind][0];\n",
       "        var tooltip = mpl.toolbar_items[toolbar_ind][1];\n",
       "        var image = mpl.toolbar_items[toolbar_ind][2];\n",
       "        var method_name = mpl.toolbar_items[toolbar_ind][3];\n",
       "\n",
       "        if (!name) {\n",
       "            /* Instead of a spacer, we start a new button group. */\n",
       "            if (buttonGroup.hasChildNodes()) {\n",
       "                toolbar.appendChild(buttonGroup);\n",
       "            }\n",
       "            buttonGroup = document.createElement('div');\n",
       "            buttonGroup.classList = 'btn-group';\n",
       "            continue;\n",
       "        }\n",
       "\n",
       "        button = fig.buttons[name] = document.createElement('button');\n",
       "        button.classList = 'btn btn-default';\n",
       "        button.href = '#';\n",
       "        button.title = name;\n",
       "        button.innerHTML = '<i class=\"fa ' + image + ' fa-lg\"></i>';\n",
       "        button.addEventListener('click', on_click_closure(method_name));\n",
       "        button.addEventListener('mouseover', on_mouseover_closure(tooltip));\n",
       "        buttonGroup.appendChild(button);\n",
       "    }\n",
       "\n",
       "    if (buttonGroup.hasChildNodes()) {\n",
       "        toolbar.appendChild(buttonGroup);\n",
       "    }\n",
       "\n",
       "    // Add the status bar.\n",
       "    var status_bar = document.createElement('span');\n",
       "    status_bar.classList = 'mpl-message pull-right';\n",
       "    toolbar.appendChild(status_bar);\n",
       "    this.message = status_bar;\n",
       "\n",
       "    // Add the close button to the window.\n",
       "    var buttongrp = document.createElement('div');\n",
       "    buttongrp.classList = 'btn-group inline pull-right';\n",
       "    button = document.createElement('button');\n",
       "    button.classList = 'btn btn-mini btn-primary';\n",
       "    button.href = '#';\n",
       "    button.title = 'Stop Interaction';\n",
       "    button.innerHTML = '<i class=\"fa fa-power-off icon-remove icon-large\"></i>';\n",
       "    button.addEventListener('click', function (_evt) {\n",
       "        fig.handle_close(fig, {});\n",
       "    });\n",
       "    button.addEventListener(\n",
       "        'mouseover',\n",
       "        on_mouseover_closure('Stop Interaction')\n",
       "    );\n",
       "    buttongrp.appendChild(button);\n",
       "    var titlebar = this.root.querySelector('.ui-dialog-titlebar');\n",
       "    titlebar.insertBefore(buttongrp, titlebar.firstChild);\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._remove_fig_handler = function (event) {\n",
       "    var fig = event.data.fig;\n",
       "    fig.close_ws(fig, {});\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._root_extra_style = function (el) {\n",
       "    el.style.boxSizing = 'content-box'; // override notebook setting of border-box.\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._canvas_extra_style = function (el) {\n",
       "    // this is important to make the div 'focusable\n",
       "    el.setAttribute('tabindex', 0);\n",
       "    // reach out to IPython and tell the keyboard manager to turn it's self\n",
       "    // off when our div gets focus\n",
       "\n",
       "    // location in version 3\n",
       "    if (IPython.notebook.keyboard_manager) {\n",
       "        IPython.notebook.keyboard_manager.register_events(el);\n",
       "    } else {\n",
       "        // location in version 2\n",
       "        IPython.keyboard_manager.register_events(el);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype._key_event_extra = function (event, _name) {\n",
       "    var manager = IPython.notebook.keyboard_manager;\n",
       "    if (!manager) {\n",
       "        manager = IPython.keyboard_manager;\n",
       "    }\n",
       "\n",
       "    // Check for shift+enter\n",
       "    if (event.shiftKey && event.which === 13) {\n",
       "        this.canvas_div.blur();\n",
       "        // select the cell after this one\n",
       "        var index = IPython.notebook.find_cell_index(this.cell_info[0]);\n",
       "        IPython.notebook.select(index + 1);\n",
       "    }\n",
       "};\n",
       "\n",
       "mpl.figure.prototype.handle_save = function (fig, _msg) {\n",
       "    fig.ondownload(fig, null);\n",
       "};\n",
       "\n",
       "mpl.find_output_cell = function (html_output) {\n",
       "    // Return the cell and output element which can be found *uniquely* in the notebook.\n",
       "    // Note - this is a bit hacky, but it is done because the \"notebook_saving.Notebook\"\n",
       "    // IPython event is triggered only after the cells have been serialised, which for\n",
       "    // our purposes (turning an active figure into a static one), is too late.\n",
       "    var cells = IPython.notebook.get_cells();\n",
       "    var ncells = cells.length;\n",
       "    for (var i = 0; i < ncells; i++) {\n",
       "        var cell = cells[i];\n",
       "        if (cell.cell_type === 'code') {\n",
       "            for (var j = 0; j < cell.output_area.outputs.length; j++) {\n",
       "                var data = cell.output_area.outputs[j];\n",
       "                if (data.data) {\n",
       "                    // IPython >= 3 moved mimebundle to data attribute of output\n",
       "                    data = data.data;\n",
       "                }\n",
       "                if (data['text/html'] === html_output) {\n",
       "                    return [cell, data, j];\n",
       "                }\n",
       "            }\n",
       "        }\n",
       "    }\n",
       "};\n",
       "\n",
       "// Register the function which deals with the matplotlib target/channel.\n",
       "// The kernel may be null if the page has been refreshed.\n",
       "if (IPython.notebook.kernel !== null) {\n",
       "    IPython.notebook.kernel.comm_manager.register_target(\n",
       "        'matplotlib',\n",
       "        mpl.mpl_figure_comm\n",
       "    );\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<img src=\"data:image/png;base64,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\" width=\"640\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x1e64499e308>"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib notebook\n",
    "a = [i[0] for i in plot_losses]\n",
    "b = [i[1] for i in plot_losses]\n",
    "c = [i[2] * 100 for i in plot_losses]\n",
    "plt.plot(a, '-', label = 'Training Loss')\n",
    "plt.plot(b, ':', label = 'Validation Loss')\n",
    "plt.plot(c, '.', label = 'Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss & Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T21:35:28.275113Z",
     "start_time": "2020-11-16T21:35:28.272613Z"
    }
   },
   "source": [
    "## 3.4 Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T20:27:43.803185Z",
     "start_time": "2020-11-16T20:27:43.798935Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1305,  410,  199, 1414, 2472])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Randomly select 5 sentences from the test set to test the translation results\n",
    "indices = np.random.choice(range(len(test_X)), 5)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T20:59:42.791400Z",
     "start_time": "2020-11-16T20:59:42.700645Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input        sentence:  ne dites rien a ma nana !\n",
      "model     translation： don t tell my girlfriend .\n",
      "reference translation： don t tell my girlfriend .\n",
      "Word accuracy： 100.0\n",
      "\n",
      "\n",
      "input        sentence:  il se mit au lit .\n",
      "model     translation： he went to bed bed .\n",
      "reference translation： he got into bed .\n",
      "Word accuracy： 50.0\n",
      "\n",
      "\n",
      "input        sentence:  mes voisines sont mes amies .\n",
      "model     translation： my friends are my friends .\n",
      "reference translation： my neighbors are my friends .\n",
      "Word accuracy： 87.5\n",
      "\n",
      "\n",
      "input        sentence:  je vais te rendre heureux .\n",
      "model     translation： i ll make you happy .\n",
      "reference translation： i ll make you happy .\n",
      "Word accuracy： 100.0\n",
      "\n",
      "\n",
      "input        sentence:  quelle est votre valise ?\n",
      "model     translation： which is your suitcase ?\n",
      "reference translation： which is your luggage ?\n",
      "Word accuracy： 87.5\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mk632\\Anaconda3\\envs\\pytorch-gpu\\lib\\site-packages\\ipykernel_launcher.py:56: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "for ind in indices:      \n",
    "    data = [test_X[ind]]\n",
    "    target = [test_Y[ind]]\n",
    "    #print('input id',data[0])\n",
    "    \n",
    "    input_variable = Variable(torch.LongTensor(data)).cuda() if use_cuda else Variable(torch.LongTensor(data))\n",
    "    # input_variable：batch_size, length_seq\n",
    "    target_variable = Variable(torch.LongTensor(target)).cuda() if use_cuda else Variable(torch.LongTensor(target))\n",
    "    # target_variable：batch_size, length_seq\n",
    "    #print('input_variable', target_variable.size())     #([1, 10])\n",
    "    #print('target id',target[0])\n",
    "    #print('target sentence',SentenceFromList(output_lang, target[0]))\n",
    "    #print('target_variable', target_variable.size())\n",
    "    encoder_hidden = encoder.initHidden(input_variable.size()[0])\n",
    "\n",
    "    loss = 0\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "    # encoder_outputs：batch_size, length_seq, hidden_size*direction\n",
    "    # encoder_hidden：direction*n_layer, batch_size, hidden_size\n",
    "\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]] * target_variable.size()[0]))     #target_variable.size()[0]  = batch_size\n",
    "    #print('decoder_input: ',decoder_input)\n",
    "    # decoder_input：二维tensor batch_size*1   \n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "    # decoder_hidden：direction*n_layer, batch_size, hidden_size\n",
    "    #print('decoder_hidden: ',decoder_hidden)       # 2 * 1*  32\n",
    "    # Without teacher forcing: use its own predictions as the next input\n",
    "    output_sentence = []\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "    rights = []\n",
    "    for di in range(MAX_LENGTH):      #every time-step    decoder_input has only one possibility\n",
    "        decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs)\n",
    "        #decoder_ouput：batch_size, output_size(vocab_size)\n",
    "        #print('decoder_output',decoder_output.size())\n",
    "        #print('decoder_output',decoder_output)\n",
    "        topv, topi = decoder_output.data.topk(1, dim = 1)\n",
    "        #print('topv:',topv ,'topi',topi)\n",
    "        decoder_attentions[di] = decoder_attention.data\n",
    "        ni = topi[:, 0]\n",
    "        #print('ni:', ni)\n",
    "        #print(decoder_input)\n",
    "        decoder_input = Variable(ni.unsqueeze(1))\n",
    "        #print(decoder_input)\n",
    "        ni = ni.cpu().numpy()[0]\n",
    "        output_sentence.append(ni)\n",
    "        #print('output_sentence', output_sentence)\n",
    "        # decoder_input：batch_size, length_seq\n",
    "        #print('decoder_input: ',decoder_input)\n",
    "        decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "        right = rightness(decoder_output, target_variable[:, di])\n",
    "        rights.append(right)\n",
    "    \n",
    "    sentence = SentenceFromList(output_lang, output_sentence)\n",
    "    standard = SentenceFromList(output_lang, target[0])\n",
    "    print('input        sentence: ',SentenceFromList(input_lang, data[0]))\n",
    "    print('model     translation：', sentence)\n",
    "    print('reference translation：', standard)\n",
    "    right_ratio = 1.0 * np.sum([i[0] for i in rights]) / np.sum([i[1] for i in rights])\n",
    "    print('Word accuracy：', 100.0 * right_ratio.cpu().numpy())\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T20:27:56.280982Z",
     "start_time": "2020-11-16T20:27:56.277927Z"
    }
   },
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import argmax\n",
    "from math import log \n",
    "from queue import PriorityQueue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-16T20:27:58.449435Z",
     "start_time": "2020-11-16T20:27:58.445514Z"
    }
   },
   "outputs": [],
   "source": [
    "def beam_search_decoder(data, k):     #data   beam size       \n",
    "    global sequences\n",
    "    #print(len(sequences[idx]) ,len(sequences[idx]),sequences[idx])\n",
    "    score,seq  = sequences[idx][beam]       \n",
    "    for j in range(len(data)):   #The number of loops is equal to the dictionary size \n",
    "        candidate = [(score - (data[j])),seq + [j], ]  #candidate The score of the location and the location index of the expanded storage history path\n",
    "        all_candidates.put(candidate)        #all_candidates stores the score and position index of all words in the current time-step\n",
    "        beam_candidates.put(candidate)\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the beam size\n",
    "beam_width = 3      \n",
    "\n",
    "for ind in indices:\n",
    "    data = [test_X[ind]]\n",
    "    target = [test_Y[ind]]\n",
    "    input_sentence = SentenceFromList(input_lang, data[0])\n",
    "    standard = SentenceFromList(output_lang, target[0])\n",
    "    print('input id',data[0])\n",
    "    print('input sentence',input_sentence)\n",
    "    input_variable = Variable(torch.LongTensor(data)).cuda() if use_cuda else Variable(torch.LongTensor(data))\n",
    "    # input_variable：batch_size, length_seq\n",
    "    target_variable = Variable(torch.LongTensor(target)).cuda() if use_cuda else Variable(torch.LongTensor(target))\n",
    "    # target_variable：batch_size, length_seq\n",
    "\n",
    "    encoder_hidden = encoder.initHidden(input_variable.size()[0])\n",
    "\n",
    "    loss = 0\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, encoder_hidden)\n",
    "    # encoder_outputs：batch_size, length_seq, hidden_size*direction\n",
    "    # encoder_hidden：direction*n_layer, batch_size, hidden_size\n",
    "    #print('target_variable', target_variable.size()[0])\n",
    "    decoder_input = Variable(torch.LongTensor([[SOS_token]] * target_variable.size()[0]))     #target_variable.size()[0]  = batch_size\n",
    "    # decoder_input：batch_size,   \n",
    "    decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "    print('decoder_input',decoder_input)\n",
    "    decoder_hidden = encoder_hidden\n",
    "    # decoder_hidden：direction*n_layer, batch_size, hidden_size\n",
    "    decoder_input_beam = [[decoder_input]]\n",
    "    # Without teacher forcing: use its own predictions as the next input\n",
    "    decoder_attentions = torch.zeros(max_length, max_length)\n",
    "    topk = [[]]\n",
    "    sequences = []   #Store batch_size sentences\n",
    "    for idx in range(decoder_output.size(0)):      #For every sentence of batch\n",
    "        print('now is the',idx, 'sentence in this batch：')\n",
    "        for di in range(MAX_LENGTH):            #for every time -step\n",
    "            print('now is the ',di,' word')\n",
    "            if di == 0 :\n",
    "                Beam_width = 1\n",
    "                for i in range(decoder_output.size(0)):   #  sequences  batch_size, [Probability value，sequence id ] \n",
    "                    sequences.append([[1.0,list()]])  \n",
    "            else:\n",
    "                Beam_width = beam_width\n",
    "          #  print('len(sequences[idx])', len(sequences[idx]))\n",
    "    \n",
    "            all_candidates = PriorityQueue()   #Initialize all_candidates every time-step\n",
    "            all_candidates.empty()\n",
    "            \n",
    "            for beam in range(Beam_width):     # The decoder_input and decoder_output between each beam of the same time-step are different \n",
    "                beam_candidates = PriorityQueue()   \n",
    "                beam_candidates.empty()\n",
    "                if di != 0:\n",
    "                    decoder_input = torch.tensor([decoder_input_beam[beam][1][-1]])\n",
    "           #         print('decoder_input',decoder_input)\n",
    "                    decoder_input = Variable(decoder_input.unsqueeze(1))\n",
    "         #           print('decoder_input',decoder_input)\n",
    "                decoder_input = decoder_input.cuda() if use_cuda else decoder_input\n",
    "                print('decoder_input',decoder_input)\n",
    "                topk = [[]]\n",
    "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                    decoder_input, decoder_hidden, encoder_outputs)\n",
    "                #decoder_ouput：batch_size, output_size(vocab_size)\n",
    "                #print('decoder_output',decoder_output.size())\n",
    "                print('decoder_output',decoder_output)\n",
    "         #       print('decoder_output',decoder_output.size())\n",
    "       #         print('idx: ',idx)\n",
    "\n",
    "                data = decoder_output[idx]    #The data size is output_size, which is the dictionary size. \n",
    "                                            #The probability of all words in the current time step\n",
    "                #print('data:',data)\n",
    "                #print('data_size:',len(data))\n",
    "                beam_search_decoder2(data, Beam_width)  \n",
    "            print('all_candidates_size is：', all_candidates.qsize())\n",
    "            #print(all_candidates.queue[:5])              \n",
    "            Beam_width = beam_width\n",
    "            for q in range(Beam_width):   \n",
    "                topk[0].append(all_candidates.get())                        \n",
    "            decoder_input_beam = topk[0]                       \n",
    "            #for i in range(len(decoder_input_beam)):\n",
    "                #print(decoder_input_beam[i][1][-1])    \n",
    "                #output_id = decoder_input_beam[i][1]\n",
    "                #sentence = SentenceFromList(output_lang, output_id)\n",
    "            sequences[idx] =topk[0]            \n",
    "        #show the result\n",
    "        print('input        sentence: ',input_sentence)\n",
    "        print('reference translation：', standard)\n",
    "        for i in range(len(decoder_input_beam)):\n",
    "            output_id = decoder_input_beam[i][1]\n",
    "            sentence = SentenceFromList(output_lang, output_id)\n",
    "            print('model translation', i,'：', sentence)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1.6-gpu",
   "language": "python",
   "name": "pytorch1.6-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
